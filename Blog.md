MLP Dungeon Gen. p1

I've been into ai a lot recently. I did a bunch of experimenting with open weights llms, like llama, qwen, phi, etc. I've messed around with some stable diffusion image generation. I've even done some text to speech and speech to text. But the stuff that reall interests me is the math and technology behind it. To try and get my feet wet, I've been reading the amazing [Deep Learning Book](https://www.deeplearningbook.org/) and watching Andrej Karpathy's Neural Networks: Zero to Hero [playlist](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1). I had just finished the episode on MLPs (MultiLayer Perceptrons) and I was inspired to do my own little project. 

The idea I settled on was to do classic rougelike, ascii tile based dungeon generation. Specifily I wanted to base the generation NN on the original Rouge, due to it's relitive simplicity. While perhaps a bit unexiteing, this gave me many benifiets. One, mass amount's of training data could esealy be generated by modifing Rouge's open source dungeon generation algorithm. Two, the traning data would be extremly uniform and easy to work with, due to existing on an ascii grid. Three, i wasn't able to find anything exactly like this with a quick google search, so I would be forced to figure out everything myself. Inspired, I quickly got to work.

To start, I'll give a breif overview of how the model works. To generate a dungeon, the model looks at a 3x3 grid of the prieveus tiles, and tries to guess what the next tile in the box will by. I also provide the model with the tile location, so I't can have some general spacial understanding. At first the entire map is empty. But as we slide the square along our map, tiles get generated little by little. The model is fed back it's previus tiles and slowly puts the rooms and connections togther. 

To implement this, I'm following closly the design proposed in [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), the paper referenced in Karpathy's video. The previous tiles and location data are each encoded into an n dimentional space, fed through a large hidden layer, and then pushed through softmax to get a probability distribution. This distribution is then sampled for the next tile. 

Before I go any further, I should probably give an overveiw of the dungeons we want to generate, and their common features. A typical Rogue dungeon looks like this:
```
       ------------                  -------------         ------------------   
       |..........|                  |...........|    #####+......?.....*...|   
       |......!...+##################+...........|    #    |................|   
       |..........|                  |...........+#####    |................|   
       --------+---                  ----------+--         --+---------------   
               #                               #             #                  
               ######                      #####             #########          
                    #                      #         ----------------+--------  
                 ---+-----                 #         |.......................|  
                 |.......|       ----------+------   |.......................|  
                 |...%...+###    |...............|   |.......................|  
                 |.......|  #    |...............|   |.......................|  
                 |.......|  #####+...............|   |.......................|  
                 -------+-       -----------------   -----------+-------------  
                        #                                       ####            
                        #        ------------------             ---+---------   
                        #        |................|      #######+...........|   
     ####################        |................+#######      |...........|   
     ############                |................|             |...........|   
                #                |................|             -------------   
                #################+................|                             
                                 ------------------                             
```
The boxes are the differnt rooms and the '#' tiles are pathways between them. The '.' tiles are ground tiles, and the '|' and '-' tiles are walls. '+' represent doors, and all other symbels are differnt items that can be found in the rooms. In this example '%' is food, '?' is a magical scroll, '!' is a potion, and '*' is a gem. 

Alright, enough conceptual stuff, let's get to work. My first task was generating and storing a bunch of Rouge levels. While idealy I would cafully extract the generation code from the Rogue source code (can be found [here](https://github.com/Davidslv/rogue)), I didn't want to spend too much time on this section. So instead I just made a small change which would write the first level to a file (instead of displaying it on the screen) and exit. I then wrote a script which started the rogue game about 10000 times and the boom, I had my training data. ![alt text](screenshots/raw_data.png "Section of the raw data file")

However, just having the raw dungeon outputs wasn't enough, I need to format it. This was pretty simple. I looped through each tile in each dungeon and generated one traning example which would look something like this:
```
0245---.....}.
```
This string tells us that at location (02, 45) the previous 3x3 square was (the question mark is the tile we are trying to guess):
```
---
...
..?
```
In this case it is another floor tile '.'. The string is split into the input data and the expected output, seperated by a '}' character. This was pretty easy to occopmlish with a simple python script (written with the help of llamma3:14b).

Finaly, with my formated training data, I was able to get started on the model. My implementation started very similar to the one outlined by Karpathy with modifications for this new sitution. I started with a simple encoding dimmension of 2, and a hidden layer size of 100. I ran 10000 scolastic gradian descent steps with a minibatch size of 30 and in a few seconds, I was greeted by my beutifele output: 

![alt text](screenshots/early_gen.png "First generation!")

So not great, running the model a few more times I got something a bit better, there where actually some roomish things now. 

![alt text](screenshots/early_gen2.png "Fourth generation!")

But something was deffenantly off. It didn't take me long to realize that I had mixed up my x and y in the inferance, and a quick fix later we were doing much better (walls go the right dirrection!).

![alt text](screenshots/early_gen_fix.png "Already fixing bugs.")

Not too bad. My next step was to increase the dimmension space and hidden layer size, but before that, I wanted to take a look at the encoding space. 

![alt text](screenshots/early_gen_emb.png "Graphs are nice.")

There's some realy interesting stuff here. All the numbers are grouped togther, presumably because they repersent location and not tiles. The ground tile ('.') is for awway, perhaps because it is well defined as something to fill in rooms. The wall tiles are interisting not at all close to each other and a small bug has let my '}' flag character slip in. It's always cool to peak into the internals of a model. 

Anyway, my first change was to up the dimension space (now 30) and hidden layer size (now 400). I also doubled the number of trainings. This took a while longer to run, but did produce a better, less chaotic output:

![alt text](screenshots/high_dim_hidden.png "Graphs are nice.")

At this point it was pretty late and I needed to stop, but this was by no means a bad start. There are many glaring flaws that I'll address next time, but I wanted to take a minute to appreciate how much this very simple model has managed to learn. It knows rooms are rectangulaly shaped, sourned by walls and filled with ground tiles. It knows doors go along walls and pathways always connect to doors (well, most of the time). And it almost always places items in rooms. If you squint and just glance, it looks very similar to what we are trying to acheaive. And I think that's a pretty good stopping point for now.